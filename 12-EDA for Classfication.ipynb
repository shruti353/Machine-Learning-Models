{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for Classification Task\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset\n",
    "# For demonstration, let's use a sample dataset (replace this with your actual dataset)\n",
    "dataset = sns.load_dataset('iris')  # Using the 'iris' dataset for demonstration\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Head:\")\n",
    "print(dataset.head())\n",
    "\n",
    "# Display dataset info to check for missing values, data types, etc.\n",
    "print(\"\\nDataset Info:\")\n",
    "print(dataset.info())\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "print(\"\\nMissing Values:\")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# Handle missing values if necessary (imputation)\n",
    "# Here, we use SimpleImputer to fill missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dataset_imputed = pd.DataFrame(imputer.fit_transform(dataset.select_dtypes(include=[np.number])), columns=dataset.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "# Merge imputed numerical columns back with categorical data\n",
    "dataset[dataset_imputed.columns] = dataset_imputed\n",
    "\n",
    "# Check if missing values are handled\n",
    "print(\"\\nMissing Values After Imputation:\")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nDuplicate Rows:\")\n",
    "print(dataset.duplicated().sum())\n",
    "\n",
    "# Drop duplicate rows if any\n",
    "dataset = dataset.drop_duplicates()\n",
    "\n",
    "# EDA Visualizations\n",
    "# 1. Distribution of Target Variable ('species' is the target variable)\n",
    "sns.countplot(x='species', data=dataset)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 2. Pairplot to understand relationships between features\n",
    "sns.pairplot(dataset, hue='species')\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation heatmap to analyze relationships between numerical features\n",
    "corr_matrix = dataset.drop(columns='species').corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 4. Boxplot to check for outliers in numerical features\n",
    "sns.boxplot(x=dataset['sepal_length'])\n",
    "plt.title('Boxplot for Sepal Length')\n",
    "plt.show()\n",
    "\n",
    "# 5. Scatter plot to visualize relationships between numerical features (e.g., 'sepal_length' vs 'sepal_width')\n",
    "sns.scatterplot(x=dataset['sepal_length'], y=dataset['sepal_width'], hue=dataset['species'])\n",
    "plt.title('Scatter Plot: Sepal Length vs Sepal Width')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.show()\n",
    "\n",
    "# 6. Histogram of numerical features (e.g., 'petal_length')\n",
    "sns.histplot(dataset['petal_length'], kde=True)\n",
    "plt.title('Distribution of Petal Length')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 7. Pairplot to explore relationships between multiple features\n",
    "sns.pairplot(dataset, hue='species')\n",
    "plt.show()\n",
    "\n",
    "# 8. Heatmap for categorical features (if applicable)\n",
    "# Example with encoding 'species' (this is already numerical in this case, but for general cases):\n",
    "# encoder = pd.get_dummies(dataset['species'], drop_first=True)\n",
    "# dataset = pd.concat([dataset, encoder], axis=1)\n",
    "# dataset = dataset.drop(columns=['species'])\n",
    "\n",
    "# Check the relationship of the target variable ('species') with numerical features\n",
    "for col in dataset.select_dtypes(include=[np.number]).columns:\n",
    "    sns.violinplot(x=dataset['species'], y=dataset[col])\n",
    "    plt.title(f'Violin Plot: {col} vs Species')\n",
    "    plt.show()\n",
    "\n",
    "# Train-Test Split\n",
    "# Let's assume 'species' is the target variable for classification\n",
    "X = dataset.drop(columns=['species'])  # Drop target column\n",
    "y = dataset['species']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard Scaling for classification models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Final dataset ready for modeling\n",
    "print(\"\\nTraining Set Shape:\", X_train_scaled.shape)\n",
    "print(\"Test Set Shape:\", X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ffe4c",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset (replace with your actual dataset)\n",
    "dataset = sns.load_dataset('iris')  # Using 'iris' dataset for demonstration\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset.info())\n",
    "\n",
    "# Step 1: Correlation Matrix for Numerical Variables (Pearson's correlation)\n",
    "# Select only the numerical columns for Pearson correlation\n",
    "numerical_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix_numerical = dataset[numerical_cols].corr()\n",
    "\n",
    "# Plot the correlation matrix for numerical variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix_numerical, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Numerical Variables')\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Correlation for Categorical Variables (Cramér’s V or Chi-squared test)\n",
    "# Select only the categorical columns\n",
    "categorical_cols = dataset.select_dtypes(include=[object, 'category']).columns\n",
    "\n",
    "# Cramér’s V function (for measuring association between categorical variables)\n",
    "def cramers_v(x, y):\n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    # Apply chi-squared test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()  # total observations\n",
    "    return np.sqrt(chi2 / (n * min(contingency_table.shape) - 1))\n",
    "\n",
    "# Generate correlation matrix for categorical variables\n",
    "cramers_v_matrix = pd.DataFrame(np.ones((len(categorical_cols), len(categorical_cols))),\n",
    "                                index=categorical_cols, columns=categorical_cols)\n",
    "\n",
    "# Calculate Cramér's V between each pair of categorical variables\n",
    "for i in range(len(categorical_cols)):\n",
    "    for j in range(i, len(categorical_cols)):\n",
    "        var1 = categorical_cols[i]\n",
    "        var2 = categorical_cols[j]\n",
    "        cramers_v_matrix.loc[var1, var2] = cramers_v(dataset[var1], dataset[var2])\n",
    "\n",
    "# Plot the Cramér’s V matrix for categorical variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cramers_v_matrix.astype(float), annot=True, cmap='Blues', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Cramér’s V Correlation Matrix for Categorical Variables')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Correlation between Object (String) Variables with Numerical Features (using Label Encoding)\n",
    "# Label encode categorical columns to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "\n",
    "# Now we can calculate Pearson correlation with encoded variables\n",
    "correlation_matrix_with_encoded_objects = dataset.corr()\n",
    "\n",
    "# Plot the correlation matrix with encoded object variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix_with_encoded_objects, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix with Encoded Object Variables')\n",
    "plt.show()\n",
    "\n",
    "# Final notes:\n",
    "# - Numerical correlation (Pearson's) is used for continuous variables.\n",
    "# - Categorical correlation (Cramér’s V) is used for categorical variables.\n",
    "# - For object variables, we can encode them using LabelEncoder and then apply Pearson correlation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
